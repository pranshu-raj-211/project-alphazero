{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMvCubH6tNU//e+PMAI0bEC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pranshu-raj-211/project-alphazero/blob/main/week2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Learnings from week 2 of winter projects\n",
        "\n",
        "## 1. What I was supposed to do\n",
        "\n",
        "1. Build some neural nets using functional API, build some CNNs and RNNs as well.\n",
        "2. Review linear algebra and calculus, write down code for the math if I have time.\n",
        "3. Do a deeper dive on chapter 12 (Generative deep learning).\n",
        "4. Deploy a couple models, preferably on real data to see things break down.\n",
        "\n",
        "## 2. What I did\n",
        "\n",
        "1. Built multi input-multi output models using functional API\n",
        "2. Tried hyperparameter tuning using keras tuner\n",
        "3. Built some metrics and loss functions using keras backend\n",
        "4. Tried making a vpn server, not working currently, also tried Azure\n",
        "5. Read the GAN paper, went throught the math and algo, will check out divergence and code(need a way to sample noise, feed data into the networks)\n",
        "\n",
        "## 3. What I'll do this week\n",
        "\n",
        "1. [Divergence](https://maps.joindeltaacademy.com/?concept=58), [Expected values]() and some lin alg [strang](https://youtube.com/playlist?list=PL221E2BBF13BECF6C&si=Mmk_QYtwH6VR4IGZ)\n",
        "2. Train GANs in Keras, Pytorch\n",
        "3. Read more papers on the topic, look at what people have created using it\n",
        "4. Need stuff to do (Long weekend)\n",
        "5.\n",
        "\n",
        "## Documentation of what I learned\n",
        "\n",
        "### TL;DR\n",
        "\n",
        "- Building networks using the functional API and then tuning them is a crucial skill.\n",
        "- Unrelated to program - learning Docker and CI/CD tools (Jenkins)\n",
        "-\n"
      ],
      "metadata": {
        "id": "PxlvoAkyWTHc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2PPTEiZVWRMZ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### KL divergence\n",
        "\n",
        "[Kullback-Leibler (KL) divergence](https://www.youtube.com/watch?v=LJwtEaP2xKA), denoted DKL(P || Q) measures how different one distribution (Q) is from another (P). If Q is our theory for how the data is generated and P is the real distribution, DKL(P || Q) measures the surprise we have when observing samples from P.\n",
        "\n",
        "DKL(P || Q) = sum(p(x)*log(p(x)/q(x)))\n",
        "\n",
        "KL divergence is nonngative and will be zero only if p(x) = q(x).\n",
        "\n"
      ],
      "metadata": {
        "id": "EhrgpJC-bDSm"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "94EwCA8AgLjs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Entropy\n",
        "\n",
        "Entropy is a quantification of surprise, think of it as being something that's inversely related to probability, In other words, if probability is high, surprise is low and vice versa.\n",
        "\n",
        "Entropy is a measure of uncertainty over the value of random variables. Think about listening to a signal. If there was 0 uncertainty in the signal (you knew in advance what it would be), then it couldn't tell you anything you didn't already know - in this case the entropy of the signal would be 0. However, if you don't know what the signal might be then it has nonzero entropy.\n",
        "\n",
        "\n",
        "- Surprise = log(1/probability)\n",
        "\n",
        "- Entropy = expected value of surprise = E(surprise)\n",
        "= sum( -p(x) log(p(x)) )\n",
        "\n",
        "Entropy is used to understand the similarity or differences between two distributions. A greater difference between two distributions will lead to a lower entropy value."
      ],
      "metadata": {
        "id": "C6FcUMGnYLNU"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rMeDWAqda_ln"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Some more topics to take care of\n",
        "\n",
        "- Mutual information\n",
        "- JS Divergence\n",
        "- Wasserstein distance\n",
        "- proofs (optimality, convergence)\n",
        "- training GANs"
      ],
      "metadata": {
        "id": "85rTgimalgXi"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gfhkVdwymL-H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Some great resources I found\n",
        "\n",
        "- https://livebook.manning.com/book/gans-in-action/chapter-1/\n",
        "- https://jonathan-hui.medium.com/gan-some-cool-applications-of-gans-4c9ecca35900\n",
        "- https://ermongroup.github.io/cs228-notes/\n",
        "- https://jonathan-hui.medium.com/gan-why-it-is-so-hard-to-train-generative-advisory-networks-819a86b3750b\n",
        "- https://developers.google.com/machine-learning/gan/generative\n",
        "- https://github.com/maxim5/cs229-2018-autumn"
      ],
      "metadata": {
        "id": "5R23K2DeADlT"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "A4qaRKNTPQ5o"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}